{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Anomaly Detector Demo\n",
    "\n",
    "## Sample Description\n",
    "### SKAB Anomaly Detection Benchmark\n",
    "- source: [Benckmark - Anomaly in Timeseries SKAB\n",
    "](https://www.kaggle.com/datasets/caesarlupum/benckmark-anomaly-timeseries-skab)\n",
    "- datasets\n",
    "    - datetime - Represents dates and times of the moment when the value is written to the database (YYYY-MM-DD hh:mm:ss)\n",
    "    - Accelerometer1RMS - Shows a vibration acceleration (Amount of g units)\n",
    "    - Accelerometer2RMS - Shows a vibration acceleration (Amount of g units)\n",
    "    - Current - Shows the amperage on the electric motor (Ampere)\n",
    "    - Pressure - Represents the pressure in the loop after the water pump (Bar)\n",
    "    - Temperature - Shows the temperature of the engine body (The degree Celsius)\n",
    "    - Thermocouple - Represents the temperature of the fluid in the circulation loop (The degree Celsius)\n",
    "    - Voltage - Shows the voltage on the electric motor (Volt)\n",
    "    - RateRMS - Represents the circulation flow rate of the fluid inside the loop (Liter per minute)\n",
    "    - anomaly - Shows if the point is anomalous (0 or 1)\n",
    "    - changepoint - Shows if the point is a changepoint for collective anomalies (0 or 1)\n",
    "\n",
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas azure-ai-anomalydetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "Load the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime as dt\n",
    "from keys import *\n",
    "# This is to build interactive plot:\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "df = pd.read_csv(\"./alldata_skab.csv\", parse_dates=['datetime'], index_col=\"datetime\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.to_series().apply(lambda x: x.date()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data transformation and dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data we want and split it into train set and test set\n",
    "df.index = df.index.to_series().apply(lambda x: x.isoformat() + \"Z\")\n",
    "df = df[df.index.str.endswith(\"0Z\") | df.index.str.endswith(\"5Z\")]\n",
    "train_df = df[df.index.str.contains(\"2020-02-08\")].copy()\n",
    "test_df = df[df.index.str.contains(\"2020-03-01\")].copy()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the redundent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['anomaly', 'changepoint'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the service client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.anomalydetector import AnomalyDetectorClient\n",
    "from azure.ai.anomalydetector.models import DetectionRequest, ModelInfo, LastDetectionRequest, VariableValues\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "anomaly_detector_endpoint = mvad_endpoint\n",
    "subscription_key = mvad_key\n",
    "# Create an Anomaly Detector client.\n",
    "ad_client = AnomalyDetectorClient(AzureKeyCredential(subscription_key), anomaly_detector_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function for preparing the data source:\n",
    "- pack_data - Pack each column into a single csv file and store as required\n",
    "- zip_file - A helper function to compress local csv files.\n",
    "- upload -  A helper function to upload files to blob\n",
    "- generate_data_source_sas - A helper function to generate blob SAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "BLOB_SAS_TEMPLATE = \"https://{account_name}.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n",
    "\n",
    "def pack_data(df, folder_name):\n",
    "    \"\"\"\n",
    "    Pack each column into a single csv file and store as the following structure:\n",
    "    - series\n",
    "        - series_1.csv\n",
    "        - series_2.csv\n",
    "        ...\n",
    "    :param df: the cleaned dataframe containing only the required variables with timestamp indices\n",
    "    :param folder_name: name of the folder of the packed files\n",
    "    \"\"\"\n",
    "\n",
    "    series = {}\n",
    "    for col in df.columns:\n",
    "        series[col] = df[[col]].copy()\n",
    "        series[col]['timestamp'] = series[col].index\n",
    "        series[col].columns = ['value', 'timestamp']\n",
    "\n",
    "    os.mkdir(folder_name)\n",
    "    for k, v in series.items():\n",
    "        v.to_csv(f\"./{folder_name}/{k}.csv\", index=False)\n",
    "\n",
    "def zip_file(root, name):\n",
    "    \"\"\"\n",
    "    A helper function to compress local csv files.\n",
    "    :param root: root directory of csv files\n",
    "    :param name: name of the compressed file (with suffix) \n",
    "    \"\"\"\n",
    "    z = zipfile.ZipFile(name, \"w\", zipfile.ZIP_DEFLATED)\n",
    "    for f in os.listdir(root):\n",
    "        if f.endswith(\"csv\"):\n",
    "            z.write(os.path.join(root, f), f)\n",
    "    z.close()\n",
    "    print(\"Compress files success!\")\n",
    "\n",
    "def upload_to_blob(file, conn_str, container, blob_name):\n",
    "    \"\"\"\n",
    "    A helper function to upload files to blob\n",
    "    :param file: the path to the file to be uploaded\n",
    "    :param conn_str: the connection string of the target storage account\n",
    "    :param container: the container name in the storage account\n",
    "    :param blob_name: the blob name in the container\n",
    "    \"\"\"\n",
    "    blob_client = BlobClient.from_connection_string(conn_str, container_name=container, blob_name=blob_name)\n",
    "    with open(file, \"rb\") as f:\n",
    "        blob_client.upload_blob(f, overwrite=True)\n",
    "    print(\"Upload Success!\")\n",
    "\n",
    "def generate_data_source_sas(conn_str, container, blob_name):\n",
    "    \"\"\"\n",
    "    A helper function to generate blob SAS.\n",
    "    :param conn_str: the connection string of the target storage account\n",
    "    :param container: the container name in the storage account\n",
    "    :param blob_name: the blob name in the container\n",
    "    :return: generated SAS\n",
    "    \"\"\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "    sas_token = generate_blob_sas(account_name=blob_service_client.account_name,\n",
    "                                  container_name=container,\n",
    "                                  blob_name=blob_name,\n",
    "                                  account_key=blob_service_client.credential.account_key,\n",
    "                                  permission=BlobSasPermissions(read=True),\n",
    "                                  expiry=datetime.utcnow() + timedelta(days=1))\n",
    "    blob_sas = BLOB_SAS_TEMPLATE.format(account_name=blob_service_client.account_name,\n",
    "                                        container_name=container,\n",
    "                                        blob_name=blob_name,\n",
    "                                        sas_token=sas_token)\n",
    "    return blob_sas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack the data and zip them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"series\"\n",
    "zipfile_name = \"series.zip\"\n",
    "\n",
    "pack_data(train_df, folder_name)\n",
    "zip_file(folder_name, zipfile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the zip file to Blob Storage and get the SAS URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = storage_account\n",
    "container_name = \"skab\"\n",
    "\n",
    "upload_to_blob(zipfile_name, connection_string, container_name, zipfile_name)\n",
    "data_source = generate_data_source_sas(connection_string, container_name, zipfile_name)\n",
    "\n",
    "print(\"Blob SAS url: \" + data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Training\n",
    "Generate data feed and start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = train_df.index.min(), train_df.index.max()\n",
    "sliding_window = 100\n",
    "data_feed = ModelInfo(start_time=start_time, end_time=end_time, source=data_source, sliding_window=sliding_window)\n",
    "response_header = ad_client.train_multivariate_model(data_feed, cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "trained_model_id = response_header['Location'].split(\"/\")[-1]\n",
    "\n",
    "print(f\"model id: {trained_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the returned model ID, we can examine its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_status = ad_client.get_multivariate_model(trained_model_id).model_info.status\n",
    "print(f\"model status: {model_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model information and track training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "model = ad_client.get_multivariate_model(trained_model_id)\n",
    "current_epoch = 0 if len(model.model_info.diagnostics_info.model_state.epoch_ids) == 0 else model.model_info.diagnostics_info.model_state.epoch_ids[-1]\n",
    "print(f\"training progress: {current_epoch}/100.\")\n",
    "if model.model_info.status == \"READY\":\n",
    "    model_state = model.model_info.diagnostics_info.model_state\n",
    "    epoch_ids = model_state.epoch_ids\n",
    "    train_losses = model_state.train_losses\n",
    "    validation_losses = model_state.validation_losses\n",
    "    latency = model_state.latencies_in_seconds\n",
    "    loss_summary = pd.DataFrame({\n",
    "        \"epoch_id\": epoch_ids, \n",
    "        \"train_loss\": train_losses, \n",
    "        \"validation_loss\": validation_losses,\n",
    "        \"latency\": latency\n",
    "    })\n",
    "    display(loss_summary)\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=train_losses, \n",
    "                             mode='lines',\n",
    "                             name='train losses'))\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=validation_losses,\n",
    "                             mode='lines',\n",
    "                             name='validation losses'))\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=latency,\n",
    "                             mode='markers', name='latency'),\n",
    "                  secondary_y=True)\n",
    "    fig.update_layout(\n",
    "        title_text=\"Visualization of training progress\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Epoch IDs\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"Loss value\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Latency (s)\", secondary_y=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference\n",
    "two ways of detecting: asynchronous and synchronous\n",
    "- Asynchronously  \n",
    "  \n",
    "  repeat the data preparation stage with different set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"test_series\"\n",
    "zipfile_name = \"test_series.zip\"\n",
    "\n",
    "pack_data(test_df.drop(['anomaly', 'changepoint'], axis=1), folder_name)\n",
    "zip_file(folder_name, zipfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = storage_account\n",
    "container_name = \"skab\"\n",
    "\n",
    "upload_to_blob(zipfile_name, connection_string, container_name, zipfile_name)\n",
    "data_source = generate_data_source_sas(connection_string, container_name, zipfile_name)\n",
    "\n",
    "print(\"Blob SAS url: \" + data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create request object with `DetectionRequest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inference_time = test_df.index.min()\n",
    "end_inference_time = test_df.index.max()\n",
    "detection_req = DetectionRequest(source=data_source, start_time=start_inference_time, end_time=end_inference_time)\n",
    "response_header = ad_client.detect_anomaly(trained_model_id, detection_req, cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "result_id = response_header['Location'].split(\"/\")[-1]\n",
    "print(f\"result id: {result_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "send the detection request with `ad_client.detect_anomaly`, and get the result id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ad_client.get_detection_result(result_id)\n",
    "print(f\"result status: {r.summary.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and Evaluation\n",
    "process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "results = r.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anomalies = []\n",
    "sev = []\n",
    "scores = []\n",
    "sensitivity = 0.7\n",
    "for item in results:\n",
    "    if item.value:\n",
    "        is_anomalies.append(item.value.is_anomaly)\n",
    "        sev.append(item.value.severity)\n",
    "        scores.append(item.value.score)\n",
    "\n",
    "anomolous_timestamps = []\n",
    "num_contributors = 3\n",
    "top_values = {f\"top_{i}\": [] for i in range(num_contributors)}\n",
    "for ts, item in zip(test_df.index, r.results):\n",
    "    if item.value.is_anomaly and item.value.severity > 1 - sensitivity:\n",
    "        anomolous_timestamps.append(ts)\n",
    "        for i in range(num_contributors):\n",
    "            top_values[f\"top_{i}\"].append(test_df[item.value.interpretation[i].variable][ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate its performance\n",
    "- extract the label and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df.anomaly.fillna(0).apply(lambda x: bool(x)).to_list()\n",
    "y_pred = [True if(is_anomalies[i] and (sev[i] > 1 - sensitivity)) else False for i in range(len(is_anomalies))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the precision, recall and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from pprint import pprint\n",
    "performance = {\n",
    "    \"precision\": accuracy_score(y_test, y_pred), \n",
    "    \"recall\": recall_score(y_test, y_pred), \n",
    "    \"f1 score\": f1_score(y_test, y_pred)}\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the series and mark out the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "colors = [px.colors.sequential.Greys[-1], px.colors.sequential.Greys[-3], px.colors.sequential.Greys[-6]]\n",
    "for v in test_df.columns:\n",
    "    if v == \"datetime\":\n",
    "        continue\n",
    "    fig.add_trace(go.Scatter(x=test_df.index, y=test_df[v], \n",
    "                             mode='lines',\n",
    "                             name=v),\n",
    "                  row=1, col=1)\n",
    "for i in range(num_contributors):\n",
    "    fig.add_trace(go.Scatter(x=anomolous_timestamps, y=top_values[f\"top_{i}\"],\n",
    "                             mode=\"markers\", name=f\"Top {i+1} contributor\",\n",
    "                             marker=dict(\n",
    "                                color=colors[i],\n",
    "                                size=8,\n",
    "                            )),\n",
    "                  row=1, col=1)\n",
    "\n",
    "y_test_timestamp = []\n",
    "y_test_score = []\n",
    "for idx, is_anomaly in enumerate(y_test):\n",
    "    if is_anomaly:\n",
    "        y_test_timestamp.append(test_df.index[idx])\n",
    "        y_test_score.append(scores[idx])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=scores,\n",
    "                         mode='lines',\n",
    "                         name='score'),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=y_test_timestamp, y=y_test_score, mode=\"markers\",  marker=dict(color=\"red\", size=8), name=\"label\"), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=sev,\n",
    "                         mode='lines', name='severity'),\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Visualization of detection results\"\n",
    ")\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"severity\", row=3, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Anomalies Synchronously\n",
    "- prepare the input as follows:\n",
    "\n",
    "  ```python\n",
    "        [{\n",
    "            \"name\": <series1>,\n",
    "            \"timestamps\": [<timestamp>],\n",
    "            \"values\": [<value>]\n",
    "         },\n",
    "          ...\n",
    "        ]\n",
    "  ```\n",
    " - create request object with `LastDetectionRequest`\n",
    " - send the detection request with `ad_client.last_detect_anomaly`, and get the result object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sample_input_df = test_df.iloc[300:410].drop(['anomaly', 'changepoint'], axis=1)\n",
    "sample_input = [{\"name\": var, \n",
    "                 \"timestamps\": sample_input_df.index.tolist(), \n",
    "                 \"values\": sample_input_df[var].tolist()} for var in sample_input_df.columns]\n",
    "last_detection_request = LastDetectionRequest(variables=[VariableValues(**item) for item in sample_input], detecting_points=10)\n",
    "res = ad_client.last_detect_anomaly(model_id=trained_model_id, body=last_detection_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the results and display the series separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0.7\n",
    "anomolous_timestamps = []\n",
    "num_contributors = 3\n",
    "anomaly_contributor = {k: {n+1: {\"timestamp\": [], \"value\": []} for n in range(num_contributors)} for k in sample_input_df.columns}\n",
    "\n",
    "for item in res.results:\n",
    "    ts = item.timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    if item.value.is_anomaly and item.value.severity > 1 - sensitivity:\n",
    "        anomolous_timestamps.append(ts)\n",
    "        for i in range(num_contributors):\n",
    "            var = item.value.interpretation[i].variable\n",
    "            anomaly_contributor[var][i+1]['timestamp'].append(ts)\n",
    "            anomaly_contributor[var][i+1]['value'].append(sample_input_df[var][ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=len(sample_input_df.columns), cols=1, shared_xaxes=True)\n",
    "colors = [px.colors.sequential.Greys[-1], px.colors.sequential.Greys[-3], px.colors.sequential.Greys[-6]]\n",
    "for idx, v in enumerate(sample_input_df.columns):\n",
    "    fig.add_trace(go.Scatter(x=sample_input_df.index, y=sample_input_df[v], \n",
    "                             mode='lines',\n",
    "                             name=v),\n",
    "                  row=idx+1, col=1)\n",
    "    for i in range(num_contributors):\n",
    "        fig.add_trace(go.Scatter(x=anomaly_contributor[v][i+1]['timestamp'], y=anomaly_contributor[v][i+1]['value'],\n",
    "                                mode=\"markers\", name=f\"Top {i+1} contributor\",\n",
    "                                marker=dict(\n",
    "                                    color=colors[i],\n",
    "                                    size=8,\n",
    "                                )),\n",
    "                    row=idx+1, col=1)\n",
    "fig.update_layout(\n",
    "    title_text=\"Visualization of detection results\"\n",
    ")\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"severity\", row=3, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f4f0ad9b6c386936399a73401713c00a20ac9e14b9e1f19ec28b5c2b0e51378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
